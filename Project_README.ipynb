{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team CarNage-2.0\n",
    "This is a submission towards the capstone for Udacity's Self Driving Car Nanodegree by Team CarNage-2.0\n",
    "\n",
    "## Team members \n",
    "The group was comprised of three members spread across the world:\n",
    "\n",
    "**Team Lead:** Sahil Malhotra (India) (sahilmalhotra17@gmail.com)\n",
    "**Team Member 1:** Marc Puig (Barcelona) (marc.puig@gmail.com)\n",
    "**Team Member 2:** Kevin Palm (United States) (kevinpalm@hotmail.com)\n",
    "\n",
    "## Project Pipeline\n",
    "We Followed the instructions of the classroom project and completed the following modules:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Waypoint updater\n",
    "The waypoint updater subscribes to the topics of base waypoints, current_pose, current velocity, traffic_waypoint and computes the final waypoints based on the current status of each of the information elements (topics) described above. \n",
    "\n",
    "**Pipeline:**\n",
    "* Update local variables based on messages received on topics\n",
    "* Create a final_waypoints message from the base waypoints; These waypoints encode target position and velocity set by the waypoint_loader\n",
    "* Based on /traffic_waypoint status. Update viewpoints to stop smoothly before the Stop lines of each waypoint\n",
    "* Publish output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. DBW Node\n",
    "The dbw_node is responsible for converting twist cmd to steer brake and throttle commands to be sent to simulator bridge or Carla. \n",
    "\n",
    "**Pipeline:**\n",
    "* Initialize PID controllers to achieve target throttle, brake and steering angles\n",
    "* Update variables based on messages received on topics\n",
    "* Calculate error between current and target velocities and orientations\n",
    "* Map the output to throttle brake and steering values\n",
    "* If /vehicle/dbw_enabled is true: run a PID iteration to compute new throttle, brake and steering values\n",
    "* Publish output\n",
    "\n",
    "**Note:** The PIDs are tuned keeping in mind the lag issues we had on our development machine so they converge just before the target value of speed. This allowed us to still stay under the speed limit even in the presence of small lag spikes due to performance issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Traffic Light Detector Node\n",
    "This node is responsible for receiving images from the camera and detecting and classifying images for traffic light status. We also repurposed the code to generate Training data from the simulator to train our deep_learning based TL Detector and Classifier described in the subsequent sections\n",
    "\n",
    "**Pipeline (Generate Training Data):**\n",
    "* Subscribe to topics and update local variables\n",
    "* Using received image message and the /vehicle/traffic_lights message values, calculate the pixel co-ordinates of traffic light bounding box\n",
    "* Record bounding box annotation and image to permanent storage for training the classifier\n",
    "* Publish traffic light ground truth state based on /vehicle/traffic lights\n",
    "\n",
    "**Pipeline (Prediction):**\n",
    "* Load frozen model\n",
    "* Subscribe to topics and update local variables\n",
    "* Convert image msg to numpy array usable by model\n",
    "* Predict using loaded model\n",
    "* Publish traffic light state based on predicted output if score of detection > 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation from Simulator\n",
    "Ultimately our team employed a few thousand training images created using the Udacity system integration simulator in our final traffic light classifier. These training files were generated in the format of an unzoomed 800x600 pixel image plus an annotation JSON which detailed the current state of the captured traffic light as well as coordinates for a rectangular bounding box surrounding the light.\n",
    "\n",
    "### Challenges\n",
    "The team faced two primary challenges while creating training data from the simulator:\n",
    "* Latency - lag between the graphical display and ROS framework resulted in incorrectly labeled frames or poorly drawn bounding boxes\n",
    "* Zooming - the team never perfectly succeeded in using simulator x,y coordinates of the vehicle and the traffic light of interest to identify which pixels in the current frame were of interest. Our final solution was good enough for training data generation, then we employed a deep neural network to locate the traffic lights when actually driving the car.\n",
    "\n",
    "### Overcoming Latency Issues\n",
    "We tried a few approaches to overcoming the poorly labeled images and poorly drawn bounding boxes created by lag, the [most novel of which probably being to try and use outlier detection techniques out of the scikit-learn module](https://github.com/raskolnikov-reborn/CarND-P13-System-Integration/blob/master/sim_data_cleanup/Simulator%20data%20cleanup.ipynb). Ultimately we ended up:\n",
    "* Combing through our ROS framework for inefficiencies to help reduce the lag in the first place (it also helped a lot that our team leader had a powerful PC to use)\n",
    "* Manual data checking\n",
    "\n",
    "### Overcoming Zooming Issues\n",
    "Our team had trouble getting the neccessary coordinate and perspective transforms to work perfectly for zooming the traffic light camera into the pixels of the traffic light of interest, [which was apparently a common problem amongst teams and is still being understood.](https://discussions.udacity.com/t/focal-length-wrong/358568) In generating training data, our solution to this was:\n",
    "* To 'tune' our zooming function with some baseline assumptions about size and proportion of the frame the through manual trial and error to be 'most correct'\n",
    "* Creating a toggle message for when the car would capture training data. This helped prevent images from being generated while the car was too far from the next traffic light and zooming would fail\n",
    "* Manual data checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Model:\n",
    "We used a faster_rcnn model and google tensorflows object detection API to train two models. One for simulator and another one for Carla.\n",
    "\n",
    "1. Sim Model: Fast RCNN pretrained on the coco dataset --> retrained on Bosch's small traffic lights data set --> Simulator generated traffic light data set. For testing generalization we only recorded images from Traffic light 1, 2, 5, 6 for training and were able to successfully detect and classify all traffic lights in the simulator during prediction\n",
    "\n",
    "2. CarlaModel: Fast RCNN pretrained on the coco dataset --> retrained on Bosch's small traffic lights data set --> rosbag generated traffic light data set. For testing generalization we only recorded images from just_traffic_lights.bag for training and were able to successfully detect and classify all traffic lights in the loop_with_traffic_light.bag. There were a few false detections where random objects in the environment were classified as traffic lights but they were very instantaneous and did not affect the performance. To test that the entire pipeline is working we launched site.launch and published a constant current velocity of 1 m/s and set the /vehicle/dbw_enabled to true by publishing a message using rostopic pub from the command line. we validated that the pipeline was working by echoing the /vehicle/brake_cmd topic on the command line. We found that when traffic light is detected as red, brake is applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "* Be sure that your workstation is running Ubuntu 16.04 Xenial Xerus or Ubuntu 14.04 Trusty Tahir. [Ubuntu downloads can be found here](https://www.ubuntu.com/download/desktop).\n",
    "\n",
    "* Follow these instructions to install ROS\n",
    "  * [ROS Kinetic](http://wiki.ros.org/kinetic/Installation/Ubuntu) if you have Ubuntu 16.04.\n",
    "  * [ROS Indigo](http://wiki.ros.org/indigo/Installation/Ubuntu) if you have Ubuntu 14.04.\n",
    "* [Dataspeed DBW](https://bitbucket.org/DataspeedInc/dbw_mkz_ros)\n",
    "  * Use this option to install the SDK on a workstation that already has ROS installed: [One Line SDK Install (binary)](https://bitbucket.org/DataspeedInc/dbw_mkz_ros/src/81e63fcc335d7b64139d7482017d6a97b405e250/ROS_SETUP.md?fileviewer=file-view-default)\n",
    "* Download the [Udacity Simulator](https://github.com/udacity/CarND-Capstone/releases/tag/v1.2).\n",
    "\n",
    "### Evaluation\n",
    "1. Clone the project repository\n",
    "```bash\n",
    "git clone https://github.com/raskolnikov-reborn/CarND-P13-System-Integration.git\n",
    "```\n",
    "\n",
    "2. Install python dependencies\n",
    "```bash\n",
    "cd CarND-P13-System-Integration\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "3. Download our pretrained models from (https://drive.google.com/file/d/0ByU68LSPjWUPUGJJWGwwN2dteE0/view?usp=sharing)\n",
    "\n",
    "4. Unzip the models.zip file \n",
    "\n",
    "5. copy the contents of models/ folder into ros/src/tl_detector/light_classification/ subdirectory of the project directory\n",
    "\n",
    "6. Make and run styx\n",
    "```bash\n",
    "cd ros\n",
    "catkin_make\n",
    "source devel/setup.sh\n",
    "roslaunch launch/styx.launch\n",
    "```\n",
    "7. Run the simulator\n",
    "\n",
    "8. Select the lowest resolution and graphics quality\n",
    "\n",
    "9. Press Start and you should see a connection accepted print in the terminal where roslaunch is running. If not, press escape and press start again\n",
    "\n",
    "10. Uncheck the Checkbox titled 'manual' on the simulator output.\n",
    "\n",
    "11. The Car should start driving while responding to traffic light status\n",
    "\n",
    "### Real world testing\n",
    "1. Launch the site.launch file\n",
    "```bash\n",
    "cd ros\n",
    "catkin_make\n",
    "source devel/setup.sh\n",
    "roslaunch launch/site.launch\n",
    "```\n",
    "2. Connect Carla\n",
    "3. Ensure all required topics are being published\n",
    "4. Enable dbw by publishing true to the /vehicle/dbw_enabled topic\n",
    "5. Confirm that traffic light detection works live images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gratitude\n",
    "1. Anthony Sarkiss for his detailed explanation on how to adapt bosch data set to the traffic light detection problem for this project: https://medium.com/@anthony_sarkis/self-driving-cars-implementing-real-time-traffic-light-detection-and-classification-in-2017-7d9ae8df1c58\n",
    "2. Jeremy Shannon: For his excellent post and tweaks on the focal length problem in the simulator which helped us generate usable training data from the simulator. https://discussions.udacity.com/t/focal-length-wrong/358568\n",
    "3. Davy for building a usable docker container before their was official docker support which helped us push development on laptop machines as well. https://discussions.udacity.com/t/docker-container-to-compile-and-run-project-linux-and-mac/362893\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
